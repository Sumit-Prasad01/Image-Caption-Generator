
# ğŸ–¼ï¸ Image Caption Generator  
**DenseNet201 + LSTM | FastAPI | Streamlit**

This project is a **production-ready Image Caption Generator** that automatically generates natural language descriptions for images using **Deep Learning**.  
It follows a **CNNâ€“RNN architecture**, where **DenseNet201** extracts visual features from images and an **LSTM-based RNN** generates captions word by word.  
The trained model is exposed via a **FastAPI backend** and consumed through an interactive **Streamlit frontend**.

---

## ğŸš€ Key Highlights

- CNNâ€“RNN based Image Captioning system
- DenseNet201 (pretrained) for robust image feature extraction
- LSTM RNN for sequential caption generation
- Complete training pipeline with configuration management
- RESTful inference API using FastAPI
- Interactive web interface built with Streamlit
- Modular, scalable, and industry-style project structure
- Model artifacts and checkpoints stored for reuse

---

## ğŸ§  Tech Stack

- **Python**
- **Deep Learning**: DenseNet201, LSTM (RNN)
- **Frameworks**: PyTorch / TensorFlow
- **Backend**: FastAPI, Uvicorn
- **Frontend**: Streamlit
- **Utilities**: NumPy, Pandas

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ models/              # Saved trained models & checkpoints
â”œâ”€â”€ backend/                 # FastAPI backend for inference
â”œâ”€â”€ frontend/                # Streamlit web application
â”œâ”€â”€ config/                  # Model & training configurations
â”œâ”€â”€ notebooks/               # Experiments and training setup
â”œâ”€â”€ pipeline/                # End-to-end training pipeline
â”œâ”€â”€ src/                     # Model architecture and core logic
â”œâ”€â”€ utils/                   # Helper utilities
â”œâ”€â”€ main.py                  # Training pipeline entry point
â”œâ”€â”€ requirements.txt         # Project dependencies
â”œâ”€â”€ setup.py                 # Package setup
â”œâ”€â”€ pyproject.toml           # Build configuration
â”œâ”€â”€ uv.lock                  # Dependency lock file
â””â”€â”€ README.md                # Documentation
```

---

## âš™ï¸ Installation

1. **Clone the repository**
```bash
git clone <https://github.com/Sumit-Prasad01/Image-Caption-Generator.git>
cd image-caption-generator
```

2. **Create and activate virtual environment**
```bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate    # Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### ğŸ”¹ Train the Model
```bash
python main.py
```

- Extracts image features using DenseNet201  
- Trains the LSTM decoder on imageâ€“caption pairs  
- Saves trained models in `artifacts/models/`

---

### ğŸ”¹ Run FastAPI Backend
```bash
uvicorn backend.app:app --reload
```

**API Endpoint**
```
POST /predict-caption
```

Input: Image file  
Output: Generated caption (JSON)

---

### ğŸ”¹ Run Streamlit Frontend
```bash
streamlit run frontend/app.py
```

- Upload an image
- Generate captions in real time using the trained model

---

## ğŸ—ï¸ Model Architecture

- **Encoder (CNN)**: DenseNet201 (pretrained)
- **Feature Vector**: Extracted from global average pooling layer
- **Decoder (RNN)**: LSTM for word sequence prediction
- **Loss Function**: Categorical Cross-Entropy
- **Optimizer**: Adam

---

## ğŸ“Š Results

- Generates grammatically correct and context-aware captions
- Learns semantic relationships between visual objects and language
- Caption quality improves with training epochs and dataset size

---

## ğŸ”® Future Improvements

- Attention-based image captioning
- Transformer-based decoder
- BLEU, METEOR, CIDEr evaluation metrics
- Multi-language caption generation
- Dockerized deployment and CI/CD

---


